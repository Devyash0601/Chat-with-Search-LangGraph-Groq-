{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=3,doc_content_chars_max=1000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=3,doc_content_chars_max=1000)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[arxiv,wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from faiss-cpu) (2.3.5)\n",
      "Requirement already satisfied: packaging in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m472.1 kB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x12a326ba0>, search_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "\n",
    "retriever_tool = Tool(\n",
    "    name=\"langsmith-search\",\n",
    "    func=retriever.invoke,\n",
    "    description=\"Search and retrieve information about LangSmith\"\n",
    ")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=1000)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=1000)),\n",
       " Tool(name='langsmith-search', description='Search and retrieve information about LangSmith', func=<bound method BaseRetriever.invoke of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x12a326ba0>, search_kwargs={})>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
      "  Using cached groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-groq) (1.2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq<1.0.0,>=0.30.0->langchain-groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.12.5)\n",
      "Collecting sniffio (from groq<1.0.0,>=0.30.0->langchain-groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.6.2)\n",
      "Downloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
      "Using cached groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, distro, groq, langchain-groq\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [langchain-groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 groq-0.37.1 langchain-groq-1.1.1 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## eun all this tools with agents and llm models\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting packaging<25,>=23.2 (from langchainhub)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchainhub) (2.32.5)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests<3,>=2->langchainhub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests<3,>=2->langchainhub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests<3,>=2->langchainhub) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests<3,>=2->langchainhub) (2025.11.12)\n",
      "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, packaging, langchainhub\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [langchainhub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langchainhub-0.1.21 packaging-24.2 types-requests-2.32.4.20250913\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use tools when needed.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/pty.py:66: DeprecationWarning: This process (pid=5029) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (1.0.5)\n",
      "Requirement already satisfied: langchain-core>=0.1 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (1.2.5)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (0.3.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (2.12.5)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
      "Requirement already satisfied: anyio in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langchain-core>=0.1->langgraph) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic>=2.7.4->langgraph) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/komya/Desktop/GenAI-2/aienv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/68_4f5d96vg7hr3x576llpxh0000gn/T/ipykernel_5029/922545165.py:3: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x12c3b7cb0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools\n",
    ")\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTell me about LangSmith\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:393\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    395\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langgraph/prebuilt/chat_agent_executor.py:668\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.call_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    666\u001b[39m     response = cast(AIMessage, dynamic_model.invoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     response = cast(AIMessage, \u001b[43mstatic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    671\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/runnables/base.py:3143\u001b[39m, in \u001b[36minvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3140\u001b[39m # invoke all steps in sequence\n\u001b[32m   3141\u001b[39m try:\n\u001b[32m   3142\u001b[39m     for i, step in enumerate(self.steps):\n\u001b[32m-> \u001b[39m\u001b[32m3143\u001b[39m         # mark each step as a child run\n\u001b[32m   3144\u001b[39m         config = patch_config(\n\u001b[32m   3145\u001b[39m             config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n\u001b[32m   3146\u001b[39m         )\n\u001b[32m   3147\u001b[39m         with set_config_context(config) as context:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36minvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_merge_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, *configs: RunnableConfig | \u001b[38;5;28;01mNone\u001b[39;00m) -> RunnableConfig:\n\u001b[32m   5547\u001b[39m     config = merge_configs(\u001b[38;5;28mself\u001b[39m.config, *configs)\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m merge_configs(config, *(f(config) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config_factories))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/langchain_groq/chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI-2/aienv/lib/python3.14/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
      "During task with name 'agent' and id 'a07c7d28-4df8-a2fa-618b-1113f2ebc4a8'"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"human\", \"Tell me about LangSmith\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
